{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-03T05:52:30.355650Z","iopub.status.busy":"2023-07-03T05:52:30.355243Z","iopub.status.idle":"2023-07-03T05:53:48.825232Z","shell.execute_reply":"2023-07-03T05:53:48.824048Z","shell.execute_reply.started":"2023-07-03T05:52:30.355619Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting lmdb\n","  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lmdb\n","Successfully installed lmdb-1.4.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting pytorch-crf\n","  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n","Installing collected packages: pytorch-crf\n","Successfully installed pytorch-crf-0.7.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.28.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.64.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.15.1)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (5.4.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: evaluate\n","Successfully installed evaluate-0.4.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mFound existing installation: transformers 4.30.1\n","Uninstalling transformers-4.30.1:\n","  Successfully uninstalled transformers-4.30.1\n","Found existing installation: accelerate 0.12.0\n","Uninstalling accelerate-0.12.0:\n","  Successfully uninstalled accelerate-0.12.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting accelerate\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n","Installing collected packages: transformers, accelerate\n","Successfully installed accelerate-0.20.3 transformers-4.30.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n","Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install lmdb\n","!pip install transformers\n","!pip install pytorch-crf\n","!pip install evaluate\n","!pip uninstall -y transformers accelerate\n","!pip install transformers accelerate\n","!pip install pandas"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:53:48.828078Z","iopub.status.busy":"2023-07-03T05:53:48.827691Z","iopub.status.idle":"2023-07-03T05:53:51.811679Z","shell.execute_reply":"2023-07-03T05:53:51.810707Z","shell.execute_reply.started":"2023-07-03T05:53:48.828038Z"},"trusted":true},"outputs":[],"source":["import lmdb\n","from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection\n","from torch.utils.data import Dataset\n","from pathlib import Path\n","import pickle as pkl\n","\n","class LMDBDataset(Dataset):\n","    \"\"\"Creates a dataset from an lmdb file.\n","    Args:\n","        data_file (Union[str, Path]): Path to lmdb file.\n","        in_memory (bool, optional): Whether to load the full dataset into memory.\n","            Default: False.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 data_file: Union[str, Path],\n","                 in_memory: bool = False):\n","\n","        data_file = Path(data_file)\n","        if not data_file.exists():\n","            raise FileNotFoundError(data_file)\n","\n","        env = lmdb.open(str(data_file), max_readers=1, readonly=True,\n","                        lock=False, readahead=False, meminit=False)\n","\n","        with env.begin(write=False) as txn:\n","            num_examples = pkl.loads(txn.get(b'num_examples'))\n","\n","        if in_memory:\n","            cache = [None] * num_examples\n","            self._cache = cache\n","\n","        self._env = env\n","        self._in_memory = in_memory\n","        self._num_examples = num_examples\n","\n","    def __len__(self) -> int:\n","        return self._num_examples\n","\n","    def __getitem__(self, index: int):\n","        if not 0 <= index < self._num_examples:\n","            raise IndexError(index)\n","\n","        if self._in_memory and self._cache[index] is not None:\n","            item = self._cache[index]\n","        else:\n","            with self._env.begin(write=False) as txn:\n","                item = pkl.loads(txn.get(str(index).encode()))\n","                if 'id' not in item:\n","                    item['id'] = str(index)\n","                if self._in_memory:\n","                    self._cache[index] = item\n","        return item"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:53:51.813913Z","iopub.status.busy":"2023-07-03T05:53:51.813293Z","iopub.status.idle":"2023-07-03T05:53:51.820706Z","shell.execute_reply":"2023-07-03T05:53:51.819786Z","shell.execute_reply.started":"2023-07-03T05:53:51.813878Z"},"trusted":true},"outputs":[],"source":["def dataset_factory(data_file: Union[str, Path], *args, **kwargs) -> Dataset:\n","    data_file = Path(data_file)\n","    if not data_file.exists():\n","        raise FileNotFoundError(data_file)\n","    if data_file.suffix == '.lmdb':\n","        return LMDBDataset(data_file, *args, **kwargs)\n","    else:\n","        raise ValueError(f\"Unrecognized datafile type {data_file.suffix}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:53:51.824332Z","iopub.status.busy":"2023-07-03T05:53:51.823412Z","iopub.status.idle":"2023-07-03T05:53:51.836441Z","shell.execute_reply":"2023-07-03T05:53:51.835295Z","shell.execute_reply.started":"2023-07-03T05:53:51.824301Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","class SecondaryStructureDataset(Dataset):\n","\n","    def __init__(self,\n","                 data_path: Union[str, Path],\n","                 split: str,\n","                 in_memory: bool = False):\n","\n","        if split not in ('train', 'valid', 'casp12', 'ts115', 'cb513','casp14_32'):\n","            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\n","                             f\"['train', 'valid', 'casp12', \"\n","                             f\"'ts115', 'cb513']\")\n","\n","        data_path = Path(data_path)\n","        data_file = f'secondary_structure/secondary_structure_{split}.lmdb'\n","        self.data = dataset_factory(data_path / data_file, in_memory)\n","\n","    def __len__(self) -> int:\n","        return len(self.data)\n","\n","    def __getitem__(self, index: int):\n","        item = self.data[index]\n","        token_ids = item['primary']\n","        input_mask = np.ones_like(token_ids)\n","\n","        # pad with -1s because of cls/sep tokens\n","        psi = np.asarray(item['psi'], np.int64)\n","        phi = np.asarray(item['phi'], np.int64)\n","        ss3 = np.asarray(item['ss3'], np.int64)\n","        ss8 = np.asarray(item['ss8'], np.int64)\n","        output = {'input_ids': token_ids,\n","                'attention_mask': input_mask,\n","                'labels': [psi,phi,ss3,ss8]}\n","        return output"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:53:51.838341Z","iopub.status.busy":"2023-07-03T05:53:51.837858Z","iopub.status.idle":"2023-07-03T05:53:51.849622Z","shell.execute_reply":"2023-07-03T05:53:51.848685Z","shell.execute_reply.started":"2023-07-03T05:53:51.838309Z"},"trusted":true},"outputs":[],"source":["class SecondaryStructureDataset_cb513(Dataset):\n","\n","    def __init__(self,\n","                 data_path: Union[str, Path],\n","                 split: str,\n","                 in_memory: bool = False):\n","\n","        if split not in ('train', 'valid', 'casp12', 'ts115', 'cb513','casp14_32'):\n","            raise ValueError(f\"Unrecognized split: {split}. Must be one of \"\n","                             f\"['train', 'valid', 'casp12', \"\n","                             f\"'ts115', 'cb513']\")\n","\n","        data_path = Path(data_path)\n","        data_file = f'secondary_structure/secondary_structure_{split}.lmdb'\n","        self.data = dataset_factory(data_path / data_file, in_memory)\n","\n","    def __len__(self) -> int:\n","        return 496\n","\n","    def __getitem__(self, index: int):\n","        item = self.data[index]\n","        token_ids = item['primary']\n","        input_mask = np.ones_like(token_ids)\n","\n","        # pad with -1s because of cls/sep tokens\n","        psi = np.asarray(item['psi'], np.int64)\n","        phi = np.asarray(item['phi'], np.int64)\n","        ss3 = np.asarray(item['ss3'], np.int64)\n","        ss8 = np.asarray(item['ss8'], np.int64)\n","        output = {'input_ids': token_ids,\n","                'attention_mask': input_mask,\n","                'labels': [psi,phi,ss3,ss8]}\n","        return output"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:53:51.851467Z","iopub.status.busy":"2023-07-03T05:53:51.851045Z","iopub.status.idle":"2023-07-03T05:53:51.953953Z","shell.execute_reply":"2023-07-03T05:53:51.953022Z","shell.execute_reply.started":"2023-07-03T05:53:51.851422Z"},"trusted":true},"outputs":[],"source":["data_dir = r'/kaggle/input/dataset'\n","train_dataset = SecondaryStructureDataset(data_dir,'train')\n","test_ts115 = SecondaryStructureDataset(data_dir,'ts115')\n","test_casp12 = SecondaryStructureDataset(data_dir,'casp12')\n","test_cb513 = SecondaryStructureDataset_cb513(data_dir,'cb513')\n","test_casp14_32 = SecondaryStructureDataset(data_dir,'casp14_32')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:53:51.957516Z","iopub.status.busy":"2023-07-03T05:53:51.957209Z","iopub.status.idle":"2023-07-03T05:55:54.837045Z","shell.execute_reply":"2023-07-03T05:55:54.835987Z","shell.execute_reply.started":"2023-07-03T05:53:51.957482Z"},"trusted":true},"outputs":[],"source":["train_sequences=[]\n","train_labels=[]\n","\n","test_ts115_sequences=[]\n","test_ts115_labels=[]\n","\n","test_casp12_sequences=[]\n","test_casp12_labels=[]\n","\n","test_cb513_sequences=[]\n","test_cb513_labels=[]\n","\n","test_casp14_32_sequences=[]\n","test_casp14_32_labels=[]\n","for seq in range(train_dataset.__len__()):\n","    train_sequences.append(train_dataset[seq]['input_ids'])\n","    train_labels.append(train_dataset[seq]['labels'])\n","for seq in range(test_ts115.__len__()):\n","    test_ts115_sequences.append(test_ts115[seq]['input_ids'])\n","    test_ts115_labels.append(test_ts115[seq]['labels'])\n","for seq in range(test_casp12.__len__()):\n","    test_casp12_sequences.append(test_casp12[seq]['input_ids'])\n","    test_casp12_labels.append(test_casp12[seq]['labels'])\n","for seq in range(test_cb513.__len__()):\n","    test_cb513_sequences.append(test_cb513[seq]['input_ids'])\n","    test_cb513_labels.append(test_cb513[seq]['labels'])\n","for seq in range(test_casp14_32.__len__()):\n","    test_casp14_32_sequences.append(test_casp14_32[seq]['input_ids'])\n","    test_casp14_32_labels.append(test_casp14_32[seq]['labels'])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:55:54.840226Z","iopub.status.busy":"2023-07-03T05:55:54.839521Z","iopub.status.idle":"2023-07-03T05:55:54.846441Z","shell.execute_reply":"2023-07-03T05:55:54.845434Z","shell.execute_reply.started":"2023-07-03T05:55:54.840191Z"},"trusted":true},"outputs":[],"source":["check_point = r'/kaggle/input/pretrain-t33/esm2_t33_650M_UR50D'"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:55:54.848564Z","iopub.status.busy":"2023-07-03T05:55:54.848132Z","iopub.status.idle":"2023-07-03T05:56:07.867216Z","shell.execute_reply":"2023-07-03T05:56:07.866218Z","shell.execute_reply.started":"2023-07-03T05:55:54.848531Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(check_point)\n","\n","train_tokenized = tokenizer(train_sequences)\n","test_ts115_tokenized = tokenizer(test_ts115_sequences)\n","test_casp12_tokenized = tokenizer(test_casp12_sequences)\n","test_cb513_tokenized = tokenizer(test_cb513_sequences)\n","test_casp14_32_tokenized = tokenizer(test_casp14_32_sequences)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:56:07.873024Z","iopub.status.busy":"2023-07-03T05:56:07.871807Z","iopub.status.idle":"2023-07-03T05:56:12.917181Z","shell.execute_reply":"2023-07-03T05:56:12.916218Z","shell.execute_reply.started":"2023-07-03T05:56:07.872974Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","\n","train_dataset = Dataset.from_dict(train_tokenized)\n","ts115_dataset = Dataset.from_dict(test_ts115_tokenized)\n","casp12_dataset = Dataset.from_dict(test_casp12_tokenized)\n","cb513_dataset = Dataset.from_dict(test_cb513_tokenized)\n","asp14_32_dataset = Dataset.from_dict(test_casp14_32_tokenized)\n","\n","train_dataset = train_dataset.add_column(\"labels\", train_labels)\n","ts115_dataset = ts115_dataset.add_column(\"labels\", test_ts115_labels)\n","casp12_dataset = casp12_dataset.add_column(\"labels\", test_casp12_labels)\n","cb513_dataset = cb513_dataset.add_column(\"labels\", test_cb513_labels)\n","asp14_32_dataset = asp14_32_dataset.add_column(\"labels\", test_casp14_32_labels)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:56:12.919537Z","iopub.status.busy":"2023-07-03T05:56:12.918857Z","iopub.status.idle":"2023-07-03T05:56:19.694256Z","shell.execute_reply":"2023-07-03T05:56:19.693281Z","shell.execute_reply.started":"2023-07-03T05:56:12.919502Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]}],"source":["from transformers import DataCollatorForTokenClassification\n","class DataCollatorForTokenClassificationMultiLabel(DataCollatorForTokenClassification):\n","    def torch_call(self, features):\n","        # logger.info(f\"features:{features}\")\n","        import torch\n","\n","        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n","        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n","        # logger.info(f'labels:{labels}')\n","        no_labels_features = [{k: v for k, v in feature.items() if k != label_name} for feature in features]\n","        # logger.info(f'no_labels_features:{no_labels_features}')\n","        batch = self.tokenizer.pad(\n","            no_labels_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","\n","        if labels is None:\n","            return batch\n","\n","        sequence_length = batch[\"input_ids\"].shape[1]\n","        padding_side = self.tokenizer.padding_side\n","\n","        def to_list(tensor_or_iterable):\n","            if isinstance(tensor_or_iterable, torch.Tensor):\n","                return tensor_or_iterable.tolist()\n","            return list(tensor_or_iterable)\n","        all_features = []\n","        if padding_side == \"right\":\n","            for label in labels:\n","                single_features=[]\n","                for k, onelabel in enumerate(label):\n","                    if k<2:\n","                        single_features.append([to_list(onelabel) + [360] * (sequence_length - len(onelabel))])\n","                    else:\n","                        single_features.append([to_list(onelabel) + [-1] * (sequence_length - len(onelabel))])\n","                all_features.append(single_features)\n","            batch[label_name] = all_features\n","            # batch[label_name] = [\n","            #     to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n","            # ]\n","        else:\n","            batch[label_name] = [\n","                [self.label_pad_token_id] * (sequence_length - len(label)) + to_list(label) for label in labels\n","            ]\n","        batch[label_name] = torch.tensor(batch[label_name]).squeeze()\n","        \n","        return batch\n","data_collator = DataCollatorForTokenClassificationMultiLabel(tokenizer,label_pad_token_id=360)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:56:19.696161Z","iopub.status.busy":"2023-07-03T05:56:19.695810Z","iopub.status.idle":"2023-07-03T05:56:19.702932Z","shell.execute_reply":"2023-07-03T05:56:19.701623Z","shell.execute_reply.started":"2023-07-03T05:56:19.696129Z"},"trusted":true},"outputs":[],"source":["def dihedral_to_radians(angle):\n","    \"\"\" Converts angles to radians\n","    Args:\n","        angles (1D Tensor): vector with angle values\n","    \"\"\"\n","    return angle*np.pi/180\n","    \n","def arctan_dihedral(sin, cos):\n","    \"\"\" Converts sin and cos back to diheral angles\n","    Args:\n","        sin (1D Tensor): vector with sin values \n","        cos (1D Tensor): vector with cos values\n","    \"\"\"\n","    result = torch.where(cos >= 0, torch.arctan(sin/cos), torch.arctan(sin/cos)+np.pi)\n","    result = torch.where((sin <= 0) & (cos <= 0), result-np.pi*2, result)\n","    \n","    return result*180/np.pi"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:56:19.705185Z","iopub.status.busy":"2023-07-03T05:56:19.704531Z","iopub.status.idle":"2023-07-03T05:57:13.483788Z","shell.execute_reply":"2023-07-03T05:57:13.482772Z","shell.execute_reply.started":"2023-07-03T05:56:19.705153Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /kaggle/input/pretrain-t33/esm2_t33_650M_UR50D were not used when initializing EsmForTokenClassificationCRF: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing EsmForTokenClassificationCRF from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing EsmForTokenClassificationCRF from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of EsmForTokenClassificationCRF were not initialized from the model checkpoint at /kaggle/input/pretrain-t33/esm2_t33_650M_UR50D and are newly initialized: ['batch_norm.weight', 'classifier_phi.0.weight', 'lstm.weight_hh_l0_reverse', 'conv2.1.weight', 'classifier_psi.0.weight', 'lstm.weight_hh_l0', 'lstm.weight_hh_l1_reverse', 'lstm.bias_hh_l1', 'lstm.bias_hh_l0', 'lstm.bias_ih_l0', 'classifier.weight', 'batch_norm.bias', 'lstm.weight_ih_l1_reverse', 'conv1.1.weight', 'lstm.weight_ih_l0', 'classifier_phi.0.bias', 'lstm.bias_ih_l1', 'lstm.weight_ih_l0_reverse', 'classifier_ss8.0.weight', 'lstm.bias_ih_l1_reverse', 'classifier.bias', 'classifier_ss8.0.bias', 'lstm.weight_ih_l1', 'conv1.1.bias', 'lstm.bias_hh_l0_reverse', 'classifier_ss3.0.bias', 'lstm.bias_hh_l1_reverse', 'classifier_psi.0.bias', 'conv2.1.bias', 'lstm.bias_ih_l0_reverse', 'lstm.weight_hh_l1', 'classifier_ss3.0.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import EsmForTokenClassification, TrainingArguments, Trainer, EsmModel\n","from transformers import TrainingArguments, Trainer, EsmModel\n","from transformers import AutoModel\n","from transformers import modeling_outputs\n","from transformers.modeling_outputs import TokenClassifierOutput\n","from torch.nn import CrossEntropyLoss\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n","import torch.nn as nn\n"," \n","##\n","\n","class EsmForTokenClassificationCRF(EsmForTokenClassification):\n","    def __init__(self, config,n_hidden=1024):\n","        super(EsmForTokenClassificationCRF, self).__init__(config)\n","        # self.classifer = nn.Linear(config.hidden_size, config.num_labels)\n","        self.lstm = nn.LSTM(input_size=config.hidden_size+64, hidden_size=n_hidden, batch_first=True, num_layers=2, bidirectional=True, dropout=0.5)\n","        self.lstm_dropout = nn.Dropout(p=0.5)\n","        self.batch_norm = nn.BatchNorm1d(config.hidden_size+64, track_running_stats=False)\n","        self.conv1 = nn.Sequential(*[\n","            nn.Dropout(p=0.5),\n","            nn.Conv1d(in_channels=config.hidden_size, out_channels=32, kernel_size=129, padding=64),\n","            nn.ReLU(),\n","        ])\n","        self.conv2 = nn.Sequential(*[\n","            nn.Dropout(p=0.5),\n","            nn.Conv1d(in_channels=config.hidden_size, out_channels=32, kernel_size=257, padding=128),\n","            nn.ReLU(),\n","        ])\n","        self.esm = EsmModel(config, add_pooling_layer=False)\n","        self.classifier_psi = nn.Sequential(*[\n","            nn.Linear(in_features=n_hidden*2, out_features=2),\n","            nn.Tanh()\n","        ])\n","        self.classifier_phi = nn.Sequential(*[\n","            nn.Linear(in_features=n_hidden*2, out_features=2),\n","            nn.Tanh()\n","        ])\n","        self.classifier_ss8 = nn.Sequential(*[\n","            nn.Linear(in_features=n_hidden*2, out_features=8),\n","            #nn.Softmax(),\n","        ])\n","        self.classifier_ss3 = nn.Sequential(*[\n","            nn.Linear(in_features=n_hidden*2, out_features=3),\n","            #nn.Softmax(),\n","        ])   \n","        for param in self.esm.parameters():\n","            param.requires_grad = False\n","\n","    def mse(self, outputs, labels, mask):\n","        loss = torch.square(outputs - labels) * mask\n","        return torch.sum(loss) / torch.sum(mask)\n","    def psi(self, outputs, labels, mask):\n","        labels = labels.unsqueeze(2)\n","        outputs = outputs.squeeze(2)\n","        # logger.info(f'mask:{mask.shape}')\n","        # logger.info(f\"labels:{labels.shape}\")\n","        mask = mask * (labels != 360).squeeze(2).int()\n","        mask = torch.cat(2*[mask.unsqueeze(2)], dim=2)\n","        # logger.info(f\"outputs.shape:{outputs.shape}\")\n","        # logger.info(f\"mask.shape:{mask.shape}\")\n","        # logger.info(f\"labels.shape:{labels.shape}\")\n","        loss = self.mse(outputs, torch.cat((torch.sin(dihedral_to_radians(labels)), torch.cos(dihedral_to_radians(labels))), dim=2).squeeze(2), mask)\n","        return loss\n","    def phi(self, outputs, labels, mask):\n","        labels = labels.unsqueeze(2)\n","        outputs = outputs.squeeze(2)\n","        # logger.info(f\"mask_value:{mask}\")\n","        # mask:[2, 279], labels:[2, 279], outputs:[2, 279, 1]\n","        mask = mask * (labels != 360).squeeze(2).int()\n","        mask = torch.cat(2*[mask.unsqueeze(2)], dim=2)\n","        loss = self.mse(outputs, torch.cat((torch.sin(dihedral_to_radians(labels)), torch.cos(dihedral_to_radians(labels))), dim=2).squeeze(2), mask)\n","        return loss\n","    def ss3(self,outputs, labels):\n","        loss_fct = CrossEntropyLoss(ignore_index=-1)\n","        labels = labels.to(outputs.device)\n","        loss = loss_fct(outputs.contiguous().view(-1, 3),labels.contiguous().view(-1))\n","        # loss = loss_fct(outputs,labels)\n","        return loss\n","    def ss8(self,outputs,labels):\n","        loss_fct = CrossEntropyLoss(ignore_index=-1)\n","        labels = labels.to(outputs.device)\n","        loss = loss_fct(outputs.contiguous().view(-1, 8),labels.contiguous().view(-1))\n","        return loss\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n","            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.esm(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        # logger.info(f\"labels_train:{labels.shape}\")\n","        sequence_output = outputs[0]\n","        _, length, _ = sequence_output.size()\n","        lengths = torch.sum(attention_mask, dim=1).cpu().long()\n","        x = sequence_output.permute(0,2,1)\n","        r1 = self.conv1(x)\n","        r2 = self.conv2(x)\n","        x = torch.cat([x, r1, r2], dim=1)\n","        x = self.batch_norm(x)\n","        x = x.permute(0,2,1)\n","        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n","        x, _ = self.lstm(x)\n","        x, _ = pad_packed_sequence(x, total_length=length, batch_first=True)\n","        \n","        x = self.lstm_dropout(x)\n","        logits_psi = self.classifier_psi(x)\n","        logits_phi = self.classifier_phi(x)\n","        logits_ss3 = self.classifier_ss3(x)\n","        logits_ss8 = self.classifier_ss8(x)\n","        # logits  = [logits_psi, logits_phi, logits_ss3, logits_ss8]\n","        logits = torch.cat((logits_psi, logits_phi, logits_ss3, logits_ss8),dim = 2).permute(0,2,1)\n","\n","        loss = None \n","        if labels is not None:\n","            loss_psi = self.psi(logits_psi, labels[:,0,:], attention_mask)*5\n","            # logger.info(f\"loss_psi:{loss_psi}\")\n","            loss_phi = self.phi(logits_phi, labels[:,1,:], attention_mask)*5\n","            # logger.info(f\"loss_phi:{loss_phi}\")\n","            # logger.info(f\"logits_ss3:{logits_ss3}\")\n","            # logger.info(f\"labels[:,2,:]:{labels[:,2,:]}\")\n","            loss_ss3 = self.ss3(logits_ss3, labels[:,2,:])*5\n","            # logger.info(f\"loss_ss3:{loss_ss3}\")\n","            loss_ss8 = self.ss8(logits_ss8, labels[:,3,:])*1\n","            # logger.info(f\"loss_ss8:{loss_ss8}\")\n","            loss = torch.stack([loss_phi, loss_psi, loss_ss3, loss_ss8])\n","            # logger.info(f\"loss:{loss}\")\n","        if not return_dict:\n","            output_psi = (logits_psi,) + outputs[2:]\n","            output_phi = (logits_phi,) + outputs[2:]\n","            output = [output_psi,output_phi]\n","            return ((loss.sum(),) + output) if loss.sum() is not None else output\n","        return TokenClassifierOutput(\n","            loss=loss.sum(),\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","model = EsmForTokenClassificationCRF.from_pretrained(check_point)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.485796Z","iopub.status.busy":"2023-07-03T05:57:13.485421Z","iopub.status.idle":"2023-07-03T05:57:13.493077Z","shell.execute_reply":"2023-07-03T05:57:13.490842Z","shell.execute_reply.started":"2023-07-03T05:57:13.485763Z"},"trusted":true},"outputs":[],"source":["def mae(pred, labels):\n","    \"\"\" Mean absolute error\n","    Args:\n","        inputs (1D Tensor): vector with predicted numeric values\n","        labels (1D Tensor): vector with correct numeric values\n","    \"\"\"\n","    err = torch.abs(labels - pred)\n","    return torch.mean(torch.fmin(err, 360-err)).item()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.495252Z","iopub.status.busy":"2023-07-03T05:57:13.494825Z","iopub.status.idle":"2023-07-03T05:57:13.506019Z","shell.execute_reply":"2023-07-03T05:57:13.505106Z","shell.execute_reply.started":"2023-07-03T05:57:13.495220Z"},"trusted":true},"outputs":[],"source":["import time\n","import math\n","from typing import Dict,NamedTuple, Optional, Tuple, Union\n","from torch.utils.data import DataLoader\n","from torch.utils.data import IterableDataset\n","from collections.abc import Mapping"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.507300Z","iopub.status.busy":"2023-07-03T05:57:13.507044Z","iopub.status.idle":"2023-07-03T05:57:13.521816Z","shell.execute_reply":"2023-07-03T05:57:13.520875Z","shell.execute_reply.started":"2023-07-03T05:57:13.507278Z"},"trusted":true},"outputs":[],"source":["def speed_metrics(split, start_time, num_samples=None, num_steps=None):\n","    \"\"\"\n","    Measure and return speed performance metrics.\n","\n","    This function requires a time snapshot `start_time` before the operation to be measured starts and this function\n","    should be run immediately after the operation to be measured has completed.\n","\n","    Args:\n","    - split: name to prefix metric (like train, eval, test...)\n","    - start_time: operation start time\n","    - num_samples: number of samples processed\n","    \"\"\"\n","    runtime = time.time() - start_time\n","    result = {f\"{split}_runtime\": round(runtime, 4)}\n","    if num_samples is not None:\n","        samples_per_second = num_samples / runtime\n","        result[f\"{split}_samples_per_second\"] = round(samples_per_second, 3)\n","    if num_steps is not None:\n","        steps_per_second = num_steps / runtime\n","        result[f\"{split}_steps_per_second\"] = round(steps_per_second, 3)\n","    return result"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.523732Z","iopub.status.busy":"2023-07-03T05:57:13.523227Z","iopub.status.idle":"2023-07-03T05:57:13.537875Z","shell.execute_reply":"2023-07-03T05:57:13.536874Z","shell.execute_reply.started":"2023-07-03T05:57:13.523698Z"},"trusted":true},"outputs":[],"source":["class EvalLoopOutput(NamedTuple):\n","    predictions: Union[np.ndarray, Tuple[np.ndarray]]\n","    label_ids: Optional[Union[np.ndarray, Tuple[np.ndarray]]]\n","    metrics: Optional[Dict[str, float]]\n","    num_samples: Optional[int]"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.539766Z","iopub.status.busy":"2023-07-03T05:57:13.539391Z","iopub.status.idle":"2023-07-03T05:57:13.548076Z","shell.execute_reply":"2023-07-03T05:57:13.547132Z","shell.execute_reply.started":"2023-07-03T05:57:13.539736Z"},"trusted":true},"outputs":[],"source":["def has_length(dataset):\n","    \"\"\"\n","    Checks if the dataset implements __len__() and it doesn't raise an error\n","    \"\"\"\n","    try:\n","        return len(dataset) is not None\n","    except TypeError:\n","        # TypeError: len() of unsized object\n","        return False"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.551741Z","iopub.status.busy":"2023-07-03T05:57:13.551437Z","iopub.status.idle":"2023-07-03T05:57:13.560483Z","shell.execute_reply":"2023-07-03T05:57:13.559427Z","shell.execute_reply.started":"2023-07-03T05:57:13.551719Z"},"trusted":true},"outputs":[],"source":["def find_batch_size(tensors):\n","    \"\"\"\n","    Find the first dimension of a tensor in a nested list/tuple/dict of tensors.\n","    \"\"\"\n","    if isinstance(tensors, (list, tuple)):\n","        for t in tensors:\n","            result = find_batch_size(t)\n","            if result is not None:\n","                return result\n","    elif isinstance(tensors, Mapping):\n","        for key, value in tensors.items():\n","            result = find_batch_size(value)\n","            if result is not None:\n","                return result\n","    elif isinstance(tensors, torch.Tensor):\n","        return tensors.shape[0] if len(tensors.shape) >= 1 else None\n","    elif isinstance(tensors, np.ndarray):\n","        return tensors.shape[0] if len(tensors.shape) >= 1 else None"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.562419Z","iopub.status.busy":"2023-07-03T05:57:13.561912Z","iopub.status.idle":"2023-07-03T05:57:13.571735Z","shell.execute_reply":"2023-07-03T05:57:13.570822Z","shell.execute_reply.started":"2023-07-03T05:57:13.562341Z"},"trusted":true},"outputs":[],"source":["def atleast_1d(tensor_or_array: Union[torch.Tensor, np.ndarray]):\n","    if isinstance(tensor_or_array, torch.Tensor):\n","        if hasattr(torch, \"atleast_1d\"):\n","            tensor_or_array = torch.atleast_1d(tensor_or_array)\n","        elif tensor_or_array.ndim < 1:\n","            tensor_or_array = tensor_or_array[None]\n","    else:\n","        tensor_or_array = np.atleast_1d(tensor_or_array)\n","    return tensor_or_array\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.573570Z","iopub.status.busy":"2023-07-03T05:57:13.573170Z","iopub.status.idle":"2023-07-03T05:57:13.587202Z","shell.execute_reply":"2023-07-03T05:57:13.586327Z","shell.execute_reply.started":"2023-07-03T05:57:13.573540Z"},"trusted":true},"outputs":[],"source":["def torch_pad_and_concatenate(tensor1, tensor2, padding_index=-100):\n","    padding_index=360\n","    \"\"\"Concatenates `tensor1` and `tensor2` on first axis, applying padding on the second if necessary.\"\"\"\n","    tensor1 = atleast_1d(tensor1)\n","    tensor2 = atleast_1d(tensor2)\n","    # logger.info(f\"tensor1:{tensor1.shape}\")\n","    # logger.info(f\"tensor2:{tensor2.shape}\")\n","    '''2023-06-26 20:38:39,863 - __main__ - INFO - tensor1:torch.Size([16, 4, 1496])\n","2023-06-26 20:38:39,864 - __main__ - INFO - tensor2:torch.Size([5, 4, 411])'''\n","    if len(tensor1.shape) == 1 or tensor1.shape[2] == tensor2.shape[2]:\n","        return torch.cat((tensor1, tensor2), dim=0)\n","\n","    # Let's figure out the new shape\n","\n","    new_shape = (tensor1.shape[0] + tensor2.shape[0], tensor1.shape[1], max(tensor1.shape[2], tensor2.shape[2])) + tensor1.shape[3:]\n","\n","    # Now let's fill the result tensor\n","    result = tensor1.new_full(new_shape, padding_index)\n","    result[: tensor1.shape[0], : tensor1.shape[1], : tensor1.shape[2]] = tensor1\n","    result[tensor1.shape[0] :, : tensor2.shape[1], : tensor2.shape[2]] = tensor2\n","    return result\n","\n","\n","def numpy_pad_and_concatenate(array1, array2, padding_index=-100):\n","    \"\"\"Concatenates `array1` and `array2` on first axis, applying padding on the second if necessary.\"\"\"\n","    array1 = atleast_1d(array1)\n","    array2 = atleast_1d(array2)\n","\n","    if len(array1.shape) == 1 or array1.shape[1] == array2.shape[1]:\n","        return np.concatenate((array1, array2), axis=0)\n","\n","    # Let's figure out the new shape\n","    new_shape = (array1.shape[0] + array2.shape[0], max(array1.shape[1], array2.shape[1])) + array1.shape[2:]\n","\n","    # Now let's fill the result tensor\n","    result = np.full_like(array1, padding_index, shape=new_shape)\n","    result[: array1.shape[0], : array1.shape[1]] = array1\n","    result[array1.shape[0] :, : array2.shape[1]] = array2\n","    return result"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.589188Z","iopub.status.busy":"2023-07-03T05:57:13.588601Z","iopub.status.idle":"2023-07-03T05:57:13.602077Z","shell.execute_reply":"2023-07-03T05:57:13.601138Z","shell.execute_reply.started":"2023-07-03T05:57:13.589153Z"},"trusted":true},"outputs":[],"source":["def nested_concat(tensors, new_tensors, padding_index=-100):\n","    \"\"\"\n","    Concat the `new_tensors` to `tensors` on the first dim and pad them on the second if needed. Works for tensors or\n","    nested list/tuples/dict of tensors.\n","    \"\"\"\n","    assert type(tensors) == type(\n","        new_tensors\n","    ), f\"Expected `tensors` and `new_tensors` to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\n","    if isinstance(tensors, (list, tuple)):\n","        return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n","    elif isinstance(tensors, torch.Tensor):\n","        return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n","    elif isinstance(tensors, Mapping):\n","        return type(tensors)(\n","            {k: nested_concat(t, new_tensors[k], padding_index=padding_index) for k, t in tensors.items()}\n","        )\n","    elif isinstance(tensors, np.ndarray):\n","        return numpy_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n","    else:\n","        raise TypeError(f\"Unsupported type for concatenation: got {type(tensors)}\")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.603949Z","iopub.status.busy":"2023-07-03T05:57:13.603508Z","iopub.status.idle":"2023-07-03T05:57:13.615377Z","shell.execute_reply":"2023-07-03T05:57:13.614497Z","shell.execute_reply.started":"2023-07-03T05:57:13.603813Z"},"trusted":true},"outputs":[],"source":["def nested_numpify(tensors):\n","    \"Numpify `tensors` (even if it's a nested list/tuple/dict of tensors).\"\n","    if isinstance(tensors, (list, tuple)):\n","        return type(tensors)(nested_numpify(t) for t in tensors)\n","    if isinstance(tensors, Mapping):\n","        return type(tensors)({k: nested_numpify(t) for k, t in tensors.items()})\n","\n","    t = tensors.cpu()\n","    if t.dtype == torch.bfloat16:\n","        # As of Numpy 1.21.4, NumPy does not support bfloat16 (see\n","        # https://github.com/numpy/numpy/blob/a47ecdea856986cd60eabbd53265c2ca5916ad5d/doc/source/user/basics.types.rst ).\n","        # Until Numpy adds bfloat16, we must convert float32.\n","        t = t.to(torch.float32)\n","    return t.numpy()"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.617354Z","iopub.status.busy":"2023-07-03T05:57:13.616836Z","iopub.status.idle":"2023-07-03T05:57:13.633647Z","shell.execute_reply":"2023-07-03T05:57:13.632658Z","shell.execute_reply.started":"2023-07-03T05:57:13.617324Z"},"trusted":true},"outputs":[],"source":["class IterableDatasetShard(IterableDataset):\n","    \"\"\"\n","    Wraps a PyTorch `IterableDataset` to generate samples for one of the processes only. Instances of this class will\n","    always yield a number of samples that is a round multiple of the actual batch size (which is `batch_size x\n","    num_processes`). Depending on the value of the `drop_last` attribute, it will either stop the iteration at the\n","    first batch that would be too small or loop with indices from the beginning.\n","\n","    On two processes with an iterable dataset yielding of `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]` with a batch size of\n","    2:\n","\n","    - the shard on process 0 will yield `[0, 1, 4, 5, 8, 9]` so will see batches `[0, 1]`, `[4, 5]`, `[8, 9]`\n","    - the shard on process 1 will yield `[2, 3, 6, 7, 10, 11]` so will see batches `[2, 3]`, `[6, 7]`, `[10, 11]`\n","\n","    <Tip warning={true}>\n","\n","        If your IterableDataset implements some randomization that needs to be applied the same way on all processes\n","        (for instance, a shuffling), you should use a `torch.Generator` in a `generator` attribute of the `dataset` to\n","        generate your random numbers and call the [`~trainer_pt_utils.IterableDatasetShard.set_epoch`] method of this\n","        object. It will set the seed of this `generator` to `seed + epoch` on all processes before starting the\n","        iteration. Alternatively, you can also implement a `set_epoch()` method in your iterable dataset to deal with\n","        this.\n","\n","    </Tip>\n","\n","    Args:\n","        dataset (`torch.utils.data.IterableDataset`):\n","            The batch sampler to split in several shards.\n","        batch_size (`int`, *optional*, defaults to 1):\n","            The size of the batches per shard.\n","        drop_last (`bool`, *optional*, defaults to `False`):\n","            Whether or not to drop the last incomplete batch or complete the last batches by using the samples from the\n","            beginning.\n","        num_processes (`int`, *optional*, defaults to 1):\n","            The number of processes running concurrently.\n","        process_index (`int`, *optional*, defaults to 0):\n","            The index of the current process.\n","        seed (`int`, *optional*, defaults to 0):\n","            A random seed that will be used for the random number generation in\n","            [`~trainer_pt_utils.IterableDatasetShard.set_epoch`].\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dataset: IterableDataset,\n","        batch_size: int = 1,\n","        drop_last: bool = False,\n","        num_processes: int = 1,\n","        process_index: int = 0,\n","        seed: int = 0,\n","    ):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.drop_last = drop_last\n","        self.num_processes = num_processes\n","        self.process_index = process_index\n","        self.seed = seed\n","        self.epoch = 0\n","        self.num_examples = 0\n","\n","    def set_epoch(self, epoch):\n","        self.epoch = epoch\n","        if hasattr(self.dataset, \"set_epoch\"):\n","            self.dataset.set_epoch(epoch)\n","\n","    def __iter__(self):\n","        self.num_examples = 0\n","        if (\n","            not hasattr(self.dataset, \"set_epoch\")\n","            and hasattr(self.dataset, \"generator\")\n","            and isinstance(self.dataset.generator, torch.Generator)\n","        ):\n","            self.dataset.generator.manual_seed(self.seed + self.epoch)\n","        real_batch_size = self.batch_size * self.num_processes\n","        process_slice = range(self.process_index * self.batch_size, (self.process_index + 1) * self.batch_size)\n","\n","        first_batch = None\n","        current_batch = []\n","        for element in self.dataset:\n","            self.num_examples += 1\n","            current_batch.append(element)\n","            # Wait to have a full batch before yielding elements.\n","            if len(current_batch) == real_batch_size:\n","                for i in process_slice:\n","                    yield current_batch[i]\n","                if first_batch is None:\n","                    first_batch = current_batch.copy()\n","                current_batch = []\n","\n","        # Finished if drop_last is True, otherwise complete the last batch with elements from the beginning.\n","        if not self.drop_last and len(current_batch) > 0:\n","            if first_batch is None:\n","                first_batch = current_batch.copy()\n","            while len(current_batch) < real_batch_size:\n","                current_batch += first_batch\n","            for i in process_slice:\n","                yield current_batch[i]\n","\n","    def __len__(self):\n","        # Will raise an error if the underlying dataset is not sized.\n","        if self.drop_last:\n","            return (len(self.dataset) // (self.batch_size * self.num_processes)) * self.batch_size\n","        else:\n","            return math.ceil(len(self.dataset) / (self.batch_size * self.num_processes)) * self.batch_size"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.635438Z","iopub.status.busy":"2023-07-03T05:57:13.635037Z","iopub.status.idle":"2023-07-03T05:57:13.648749Z","shell.execute_reply":"2023-07-03T05:57:13.647901Z","shell.execute_reply.started":"2023-07-03T05:57:13.635407Z"},"trusted":true},"outputs":[],"source":["def nested_truncate(tensors, limit):\n","    \"Truncate `tensors` at `limit` (even if it's a nested list/tuple/dict of tensors).\"\n","    if isinstance(tensors, (list, tuple)):\n","        return type(tensors)(nested_truncate(t, limit) for t in tensors)\n","    if isinstance(tensors, Mapping):\n","        return type(tensors)({k: nested_truncate(t, limit) for k, t in tensors.items()})\n","\n","    return tensors[:limit]"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.650373Z","iopub.status.busy":"2023-07-03T05:57:13.649966Z","iopub.status.idle":"2023-07-03T05:57:13.661188Z","shell.execute_reply":"2023-07-03T05:57:13.660328Z","shell.execute_reply.started":"2023-07-03T05:57:13.650341Z"},"trusted":true},"outputs":[],"source":["class EvalPrediction:\n","    \"\"\"\n","    Evaluation output (always contains labels), to be used to compute metrics.\n","\n","    Parameters:\n","        predictions (`np.ndarray`): Predictions of the model.\n","        label_ids (`np.ndarray`): Targets to be matched.\n","        inputs (`np.ndarray`, *optional*)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        predictions: Union[np.ndarray, Tuple[np.ndarray]],\n","        label_ids: Union[np.ndarray, Tuple[np.ndarray]],\n","        inputs: Optional[Union[np.ndarray, Tuple[np.ndarray]]] = None,\n","    ):\n","        self.predictions = predictions\n","        self.label_ids = label_ids\n","        self.inputs = inputs\n","\n","    def __iter__(self):\n","        if self.inputs is not None:\n","            return iter((self.predictions, self.label_ids, self.inputs))\n","        else:\n","            return iter((self.predictions, self.label_ids))\n","\n","    def __getitem__(self, idx):\n","        if idx < 0 or idx > 2:\n","            raise IndexError(\"tuple index out of range\")\n","        if idx == 2 and self.inputs is None:\n","            raise IndexError(\"tuple index out of range\")\n","        if idx == 0:\n","            return self.predictions\n","        elif idx == 1:\n","            return self.label_ids\n","        elif idx == 2:\n","            return self.inputs"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.663050Z","iopub.status.busy":"2023-07-03T05:57:13.662609Z","iopub.status.idle":"2023-07-03T05:57:13.674105Z","shell.execute_reply":"2023-07-03T05:57:13.673133Z","shell.execute_reply.started":"2023-07-03T05:57:13.663011Z"},"trusted":true},"outputs":[],"source":["import importlib.util\n","_torch_available = importlib.util.find_spec(\"torch\") is not None\n","def is_torch_available():\n","    return _torch_available"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.684603Z","iopub.status.busy":"2023-07-03T05:57:13.684317Z","iopub.status.idle":"2023-07-03T05:57:13.690908Z","shell.execute_reply":"2023-07-03T05:57:13.690022Z","shell.execute_reply.started":"2023-07-03T05:57:13.684579Z"},"trusted":true},"outputs":[],"source":["def denumpify_detensorize(metrics):\n","    \"\"\"\n","    Recursively calls `.item()` on the element of the dictionary passed\n","    \"\"\"\n","    if isinstance(metrics, (list, tuple)):\n","        return type(metrics)(denumpify_detensorize(m) for m in metrics)\n","    elif isinstance(metrics, dict):\n","        return type(metrics)({k: denumpify_detensorize(v) for k, v in metrics.items()})\n","    elif isinstance(metrics, np.generic):\n","        return metrics.item()\n","    elif is_torch_available() and isinstance(metrics, torch.Tensor) and metrics.numel() == 1:\n","        return metrics.item()\n","    return metrics"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.692991Z","iopub.status.busy":"2023-07-03T05:57:13.692421Z","iopub.status.idle":"2023-07-03T05:57:13.734568Z","shell.execute_reply":"2023-07-03T05:57:13.733731Z","shell.execute_reply.started":"2023-07-03T05:57:13.692960Z"},"trusted":true},"outputs":[],"source":["from typing import Dict, List, Optional\n","\n","from torch.utils.data import Dataset\n","\n","\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        # forward pass\n","        outputs = model(**inputs)\n","        # logger. info(f\"outputs:{type(outputs)}\")\n","        logits = outputs.get(\"logits\")\n","        loss = outputs.get(\"loss\")\n","        return (loss, outputs) if return_outputs else loss\n","    def evaluate(\n","        self,\n","        eval_dataset: Optional[Dataset] = None,\n","        ignore_keys: Optional[List[str]] = None,\n","        metric_key_prefix: str = \"eval\",\n","    ) -> Dict[str, float]:\n","        \"\"\"\n","        Run evaluation and returns metrics.\n","\n","        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n","        (pass it to the init `compute_metrics` argument).\n","\n","        You can also subclass and override this method to inject custom behavior.\n","\n","        Args:\n","            eval_dataset (`Dataset`, *optional*):\n","                Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n","                not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n","                method.\n","            ignore_keys (`List[str]`, *optional*):\n","                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n","                gathering predictions.\n","            metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n","                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n","                \"eval_bleu\" if the prefix is \"eval\" (default)\n","\n","        Returns:\n","            A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n","            dictionary also contains the epoch number which comes from the training state.\n","        \"\"\"\n","        # memory metrics - must set up as early as possible\n","        self._memory_tracker.start()\n","\n","        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n","        start_time = time.time()\n","\n","        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n","        output = eval_loop(\n","            eval_dataloader,\n","            description=\"Evaluation\",\n","            # No point gathering the predictions if there are no metrics, otherwise we defer to\n","            # self.args.prediction_loss_only\n","            prediction_loss_only=True if self.compute_metrics is None else None,\n","            ignore_keys=ignore_keys,\n","            metric_key_prefix=metric_key_prefix,\n","        )\n","\n","        total_batch_size = self.args.eval_batch_size * self.args.world_size\n","        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n","            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n","        output.metrics.update(\n","            speed_metrics(\n","                metric_key_prefix,\n","                start_time,\n","                num_samples=output.num_samples,\n","                num_steps=math.ceil(output.num_samples / total_batch_size),\n","            )\n","        )\n","\n","        self.log(output.metrics)\n","\n","        # if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n","        #     # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n","        #     xm.master_print(met.metrics_report())\n","\n","        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)\n","\n","        self._memory_tracker.stop_and_update_metrics(output.metrics)\n","\n","        return output.metrics\n","    def evaluation_loop(\n","        self,\n","        dataloader: DataLoader,\n","        description: str,\n","        prediction_loss_only: Optional[bool] = None,\n","        ignore_keys: Optional[List[str]] = None,\n","        metric_key_prefix: str = \"eval\",\n","    ) -> EvalLoopOutput:\n","        \"\"\"\n","        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n","\n","        Works both with or without labels.\n","        \"\"\"\n","        args = self.args\n","\n","        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n","\n","        # if eval is called w/o train init deepspeed here\n","        # if args.deepspeed and not self.deepspeed:\n","        #     # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n","        #     # from the checkpoint eventually\n","        #     deepspeed_engine, _, _ = deepspeed_init(\n","        #         self, num_training_steps=0, resume_from_checkpoint=None, inference=True\n","        #     )\n","        #     self.model = deepspeed_engine.module\n","        #     self.model_wrapped = deepspeed_engine\n","        #     self.deepspeed = deepspeed_engine\n","\n","        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n","\n","        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n","        # while ``train`` is running, cast it to the right dtype first and then put on device\n","        if not self.is_in_train:\n","            if args.fp16_full_eval:\n","                model = model.to(dtype=torch.float16, device=args.device)\n","            elif args.bf16_full_eval:\n","                model = model.to(dtype=torch.bfloat16, device=args.device)\n","\n","        batch_size = self.args.eval_batch_size\n","\n","        # logger.info(f\"***** Running {description} *****\")\n","        # if has_length(dataloader):\n","        #     logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n","        # else:\n","        #     logger.info(\"  Num examples: Unknown\")\n","        # logger.info(f\"  Batch size = {batch_size}\")\n","\n","        model.eval()\n","\n","        self.callback_handler.eval_dataloader = dataloader\n","        # Do this before wrapping.\n","        eval_dataset = getattr(dataloader, \"dataset\", None)\n","\n","        # if is_torch_tpu_available():\n","        #     dataloader = pl.ParallelLoader(dataloader, [args.device]).per_device_loader(args.device)\n","\n","        if args.past_index >= 0:\n","            self._past = None\n","\n","        # Initialize containers\n","        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n","        losses_host = None\n","        preds_host = None\n","        labels_host = None\n","        inputs_host = None\n","\n","        # losses/preds/labels on CPU (final containers)\n","        all_losses = None\n","        all_preds = None\n","        all_labels = None\n","        all_inputs = None\n","        # Will be useful when we have an iterable dataset so don't know its length.\n","\n","        observed_num_examples = 0\n","        # Main evaluation loop\n","        for step, inputs in enumerate(dataloader):\n","            # Update the observed num examples\n","            observed_batch_size = find_batch_size(inputs)\n","            # logger.info(f\"inputs['labels']:{inputs['labels'].shape}\")\n","            # inputs['labels']:torch.Size([16, 4, 1496])inputs['labels']:torch.Size([5, 4, 411])\n","\n","            # logger.info(f\"observed_batch_size:{observed_batch_size}\")\n","            if observed_batch_size is not None:\n","                observed_num_examples += observed_batch_size\n","                # For batch samplers, batch_size is not known by the dataloader in advance.\n","                if batch_size is None:\n","                    batch_size = observed_batch_size\n","\n","            # Prediction step\n","            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n","\n","            inputs_decode = self._prepare_input(inputs[\"input_ids\"]) if args.include_inputs_for_metrics else None\n","\n","            # if is_torch_tpu_available():\n","            #     xm.mark_step()\n","\n","            # Update containers on host\n","            if loss is not None:\n","                losses = self._nested_gather(loss.repeat(batch_size))\n","                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n","            # logger.info(f\"labels.shape:{labels.shape}\")\n","\n","            if labels is not None:\n","                labels = self._pad_across_processes(labels)\n","                labels = self._nested_gather(labels)\n","                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n","            # logger.info(f\"labels_pad_across_processes:{labels}\")\n","            if inputs_decode is not None:\n","                inputs_decode = self._pad_across_processes(inputs_decode)\n","                inputs_decode = self._nested_gather(inputs_decode)\n","                inputs_host = (\n","                    inputs_decode\n","                    if inputs_host is None\n","                    else nested_concat(inputs_host, inputs_decode, padding_index=-100)\n","                )\n","            if logits is not None:\n","                logits = self._pad_across_processes(logits)\n","                logits = self._nested_gather(logits)\n","                if self.preprocess_logits_for_metrics is not None:\n","                    logits = self.preprocess_logits_for_metrics(logits, labels)\n","                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n","                # print(\"logits.shape\",logits.shape)\n","            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n","\n","            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n","            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n","                if losses_host is not None:\n","                    losses = nested_numpify(losses_host)\n","                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n","                if preds_host is not None:\n","                    logits = nested_numpify(preds_host)\n","                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n","                if inputs_host is not None:\n","                    inputs_decode = nested_numpify(inputs_host)\n","                    all_inputs = (\n","                        inputs_decode\n","                        if all_inputs is None\n","                        else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n","                    )\n","                if labels_host is not None:\n","                    labels = nested_numpify(labels_host)\n","                    all_labels = (\n","                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n","                    )\n","\n","                # Set back to None to begin a new accumulation\n","                losses_host, preds_host, inputs_host, labels_host = None, None, None, None\n","\n","        if args.past_index and hasattr(self, \"_past\"):\n","            # Clean the state at the end of the evaluation loop\n","            delattr(self, \"_past\")\n","\n","        # Gather all remaining tensors and put them back on the CPU\n","        if losses_host is not None:\n","            losses = nested_numpify(losses_host)\n","            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n","        if preds_host is not None:\n","            logits = nested_numpify(preds_host)\n","            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n","        if inputs_host is not None:\n","            inputs_decode = nested_numpify(inputs_host)\n","            all_inputs = (\n","                inputs_decode if all_inputs is None else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n","            )\n","\n","        if labels_host is not None:\n","\n","            labels = nested_numpify(labels_host)\n","            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n","\n","        # Number of samples\n","        if has_length(eval_dataset):\n","            num_samples = len(eval_dataset)\n","        # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n","        # methods. Therefore we need to make sure it also has the attribute.\n","        elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n","            num_samples = eval_dataset.num_examples\n","        else:\n","            if has_length(dataloader):\n","                num_samples = self.num_examples(dataloader)\n","            else:  # both len(dataloader.dataset) and len(dataloader) fail\n","                num_samples = observed_num_examples\n","        if num_samples == 0 and observed_num_examples > 0:\n","            num_samples = observed_num_examples\n","\n","        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n","        # samplers has been rounded to a multiple of batch_size, so we truncate.\n","        if all_losses is not None:\n","            all_losses = all_losses[:num_samples]\n","        if all_preds is not None:\n","            all_preds = nested_truncate(all_preds, num_samples)\n","        if all_labels is not None:\n","\n","            all_labels = nested_truncate(all_labels, num_samples)\n","        if all_inputs is not None:\n","            all_inputs = nested_truncate(all_inputs, num_samples)\n","\n","        # Metrics!\n","        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n","\n","            if args.include_inputs_for_metrics:\n","                metrics = self.compute_metrics(\n","                    EvalPrediction(predictions=all_preds, label_ids=all_labels, inputs=all_inputs)\n","                )\n","            else:\n","                metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n","        else:\n","            metrics = {}\n","\n","        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n","        metrics = denumpify_detensorize(metrics)\n","\n","        if all_losses is not None:\n","            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n","        if hasattr(self, \"jit_compilation_time\"):\n","            metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n","\n","        # Prefix all keys with metric_key_prefix + '_'\n","        for key in list(metrics.keys()):\n","            if not key.startswith(f\"{metric_key_prefix}_\"):\n","                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n","\n","        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.736656Z","iopub.status.busy":"2023-07-03T05:57:13.736040Z","iopub.status.idle":"2023-07-03T05:57:13.751592Z","shell.execute_reply":"2023-07-03T05:57:13.750675Z","shell.execute_reply.started":"2023-07-03T05:57:13.736626Z"},"trusted":true},"outputs":[],"source":["def accuracy(pred, labels):\n","    \"\"\" Accuracy coefficient\n","    Args:\n","        inputs (1D Tensor): vector with predicted integer values\n","        labels (1D Tensor): vector with correct integer values\n","    \"\"\"\n","    \n","    return (sum((pred == labels)) / len(labels)).item()\n","def evaluate_psi(pred, labels):\n","    mask = torch.ones_like(labels)\n","    mask = mask * (labels != 360)\n","    labels = labels[mask == 1]\n","    outputs = arctan_dihedral(pred[:,:,0], pred[:,:,1])[mask == 1]\n","    return mae(outputs, labels)\n","def evaluate_phi(pred, labels):\n","    mask = torch.ones_like(labels)\n","    mask = mask * (labels != 360)\n","    labels = labels[mask == 1]\n","    outputs = arctan_dihedral(pred[:,:,0], pred[:,:,1])[mask == 1]\n","    return mae(outputs, labels)\n","\n","def evaluate_ss3(pred, labels):\n","\n","    mask = torch.ones_like(labels)\n","    mask = mask * (labels != 360)\n","    mask = mask * (labels != -1)\n","    labels = labels[mask == 1]\n","    outputs = torch.argmax(pred, dim=2)[mask == 1]\n","        \n","    return accuracy(outputs, labels)\n","\n","def evaluate_ss8(pred, labels):\n","    mask = torch.ones_like(labels)\n","    mask = mask * (labels != 360)\n","    mask = mask * (labels != -1)\n","    labels = labels[mask == 1]\n","    outputs = torch.argmax(pred, dim=2)[mask == 1]\n","        \n","    return accuracy(outputs, labels)\n","\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = torch.tensor(predictions).permute(0,2,1)\n","    labels = torch.tensor(labels).permute(0,2,1)\n","\n","    # print('predictions_untra',predictions.shape)\n","    # print('labels_untra',labels.shape)\n","    return {\"psi\":evaluate_psi(predictions[:,:,0:2],labels[:,:,0]),\n","            \"phi\":evaluate_phi(predictions[:,:,2:4],labels[:,:,1]),\n","            \"ss3\":evaluate_ss3(predictions[:,:,4:7],labels[:,:,2]),\n","            \"ss8\":evaluate_ss8(predictions[:,:,7:15],labels[:,:,3])}"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.753368Z","iopub.status.busy":"2023-07-03T05:57:13.752829Z","iopub.status.idle":"2023-07-03T05:57:13.804738Z","shell.execute_reply":"2023-07-03T05:57:13.803823Z","shell.execute_reply.started":"2023-07-03T05:57:13.753334Z"},"trusted":true},"outputs":[],"source":["# model_name = model_checkpoint.split(\"/\")[-1]\n","model_name=\"esm2_t30_150M_UR50D\"\n","batch_size = 16\n","\n","args = TrainingArguments(\n","    f\"{model_name}-cnn-lstm-psi-psi-ss3-ss8-230628\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"no\",\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=2,\n","    weight_decay=0.001,\n","    load_best_model_at_end=False,\n","    # metric_for_best_model=\"mae\",\n","    push_to_hub=False,\n","    fp16=False,\n","    fp16_full_eval=False,\n","    # use_legacy_prediction_loop=True,\n","    # report_to=None,\n",")"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:13.806590Z","iopub.status.busy":"2023-07-03T05:57:13.806237Z","iopub.status.idle":"2023-07-03T05:57:20.441492Z","shell.execute_reply":"2023-07-03T05:57:20.440240Z","shell.execute_reply.started":"2023-07-03T05:57:13.806559Z"},"trusted":true},"outputs":[],"source":["trainer = CustomTrainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=asp14_32_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T05:57:20.443059Z","iopub.status.busy":"2023-07-03T05:57:20.442738Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.15.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230703_055908-9xqn0yay</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/luohui/huggingface/runs/9xqn0yay' target=\"_blank\">dandy-pond-61</a></strong> to <a href='https://wandb.ai/luohui/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/luohui/huggingface' target=\"_blank\">https://wandb.ai/luohui/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/luohui/huggingface/runs/9xqn0yay' target=\"_blank\">https://wandb.ai/luohui/huggingface/runs/9xqn0yay</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='682' max='1086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 682/1086 39:05 < 23:13, 0.29 it/s, Epoch 1.25/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Psi</th>\n","      <th>Phi</th>\n","      <th>Ss3</th>\n","      <th>Ss8</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>4.446600</td>\n","      <td>4.806728</td>\n","      <td>43.688942</td>\n","      <td>18.260643</td>\n","      <td>0.860170</td>\n","      <td>0.745023</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def mae1(pred, labels):\n","    \"\"\" Mean absolute error\n","    Args:\n","        inputs (1D Tensor): vector with predicted numeric values\n","        labels (1D Tensor): vector with correct numeric values\n","    \"\"\"\n","    err = torch.abs(labels - pred)\n","    # return torch.mean(torch.fmin(err, 360-err)).item()\n","    return torch.fmin(err, torch.abs(360-err))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate_psi_sep(pred, labels):\n","\n","    outputs = arctan_dihedral(pred[:,:,0], pred[:,:,1])\n","    return outputs,mae1(outputs, labels)\n","def evaluate_phi_sep(pred, labels):\n","    outputs = arctan_dihedral(pred[:,:,0], pred[:,:,1])\n","    return outputs,mae1(outputs, labels)\n","\n","def evaluate_ss3_sep(pred, labels):\n","\n","    outputs = torch.argmax(pred, dim=2)\n","        \n","    return labels,outputs\n","\n","def evaluate_ss8_sep(pred, labels):\n","    outputs = torch.argmax(pred, dim=2)\n","        \n","    return labels,outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_label(label,reference):    \n","    mask = torch.ones_like(label)\n","    mask = mask * (reference != 360)\n","    mask = mask * (reference != -1)\n","    return label[mask == 1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_original, labels_original, _ = trainer.predict(ts115_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions=predictions_original.transpose(0,2,1)\n","labels=labels_original.transpose(0,2,1)\n","predictions = torch.tensor(predictions)\n","labels = torch.tensor(labels)\n","\n","psi_result,psi_loss = evaluate_psi_sep(predictions[:,:,0:2],labels[:,:,0])\n","phi_result,phi_loss = evaluate_phi_sep(predictions[:,:,2:4],labels[:,:,1])\n","ss3_origi_label,ss3_outputs = evaluate_ss3_sep(predictions[:,:,4:7],labels[:,:,2])\n","ss8_origi_label,ss8_outputs = evaluate_ss8_sep(predictions[:,:,7:15],labels[:,:,3])\n","\n","psi_loss_view = psi_loss.reshape(-1)\n","phi_loss_view = phi_loss.reshape(-1)\n","ss3_view = ss3_origi_label.reshape(-1)\n","ss8_view = ss8_origi_label.reshape(-1)\n","ss3_res_view = ss3_outputs.reshape(-1)\n","ss8_res_view = ss8_outputs.reshape(-1)\n","\n","ss3_label_view = process_label(ss3_view,ss3_view)\n","ss8_label_view = process_label(ss8_view,ss8_view)\n","ss3_res_post = process_label(ss3_res_view,ss3_view)\n","ss8_res_post = process_label(ss8_res_view,ss8_view)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["psi_result.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ss3_outputs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# torch.save(psi_result,\"./psi_result.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# torch.save(phi_result,\"./phi_result.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# torch.save(ss3_origi_label,\"./ss3_origi_label.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# torch.save(ss8_origi_label,\"./ss8_origi_label.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["aa = {\n","    \"ss3_label\":ss3_label_view,\n","    \"ss3_result\":ss3_res_post,\n","    \"ss8_label\":ss8_label_view,\n","    \"ss8_result\":ss8_res_post\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(aa,\"./ts115_ss3_ss8_label_result.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_original, labels_original, _ = trainer.predict(asp14_32_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions=predictions_original.transpose(0,2,1)\n","labels=labels_original.transpose(0,2,1)\n","predictions = torch.tensor(predictions)\n","labels = torch.tensor(labels)\n","\n","psi_result,psi_loss = evaluate_psi_sep(predictions[:,:,0:2],labels[:,:,0])\n","phi_result,phi_loss = evaluate_phi_sep(predictions[:,:,2:4],labels[:,:,1])\n","ss3_origi_label,ss3_outputs = evaluate_ss3_sep(predictions[:,:,4:7],labels[:,:,2])\n","ss8_origi_label,ss8_outputs = evaluate_ss8_sep(predictions[:,:,7:15],labels[:,:,3])\n","\n","psi_loss_view = psi_loss.reshape(-1)\n","phi_loss_view = phi_loss.reshape(-1)\n","ss3_view = ss3_origi_label.reshape(-1)\n","ss8_view = ss8_origi_label.reshape(-1)\n","ss3_res_view = ss3_outputs.reshape(-1)\n","ss8_res_view = ss8_outputs.reshape(-1)\n","\n","ss3_label_view = process_label(ss3_view,ss3_view)\n","ss8_label_view = process_label(ss8_view,ss8_view)\n","ss3_res_post = process_label(ss3_res_view,ss3_view)\n","ss8_res_post = process_label(ss8_res_view,ss8_view)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bb = {\n","    \"ss3_label\":ss3_label_view,\n","    \"ss3_result\":ss3_res_post,\n","    \"ss8_label\":ss8_label_view,\n","    \"ss8_result\":ss8_res_post\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(bb,\"./casp14_32_ss3_ss8_label_result.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_original, labels_original, _ = trainer.predict(casp12_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions=predictions_original.transpose(0,2,1)\n","labels=labels_original.transpose(0,2,1)\n","predictions = torch.tensor(predictions)\n","labels = torch.tensor(labels)\n","\n","psi_result,psi_loss = evaluate_psi_sep(predictions[:,:,0:2],labels[:,:,0])\n","phi_result,phi_loss = evaluate_phi_sep(predictions[:,:,2:4],labels[:,:,1])\n","ss3_origi_label,ss3_outputs = evaluate_ss3_sep(predictions[:,:,4:7],labels[:,:,2])\n","ss8_origi_label,ss8_outputs = evaluate_ss8_sep(predictions[:,:,7:15],labels[:,:,3])\n","\n","psi_loss_view = psi_loss.reshape(-1)\n","phi_loss_view = phi_loss.reshape(-1)\n","ss3_view = ss3_origi_label.reshape(-1)\n","ss8_view = ss8_origi_label.reshape(-1)\n","ss3_res_view = ss3_outputs.reshape(-1)\n","ss8_res_view = ss8_outputs.reshape(-1)\n","\n","ss3_label_view = process_label(ss3_view,ss3_view)\n","ss8_label_view = process_label(ss8_view,ss8_view)\n","ss3_res_post = process_label(ss3_res_view,ss3_view)\n","ss8_res_post = process_label(ss8_res_view,ss8_view)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cc = {\n","    \"ss3_label\":ss3_label_view,\n","    \"ss3_result\":ss3_res_post,\n","    \"ss8_label\":ss8_label_view,\n","    \"ss8_result\":ss8_res_post\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(cc,\"./casp12_ss3_ss8_label_result.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions_original, labels_original, _ = trainer.predict(cb513_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions=predictions_original.transpose(0,2,1)\n","labels=labels_original.transpose(0,2,1)\n","predictions = torch.tensor(predictions)\n","labels = torch.tensor(labels)\n","\n","psi_result,psi_loss = evaluate_psi_sep(predictions[:,:,0:2],labels[:,:,0])\n","phi_result,phi_loss = evaluate_phi_sep(predictions[:,:,2:4],labels[:,:,1])\n","ss3_origi_label,ss3_outputs = evaluate_ss3_sep(predictions[:,:,4:7],labels[:,:,2])\n","ss8_origi_label,ss8_outputs = evaluate_ss8_sep(predictions[:,:,7:15],labels[:,:,3])\n","\n","psi_loss_view = psi_loss.reshape(-1)\n","phi_loss_view = phi_loss.reshape(-1)\n","ss3_view = ss3_origi_label.reshape(-1)\n","ss8_view = ss8_origi_label.reshape(-1)\n","ss3_res_view = ss3_outputs.reshape(-1)\n","ss8_res_view = ss8_outputs.reshape(-1)\n","\n","ss3_label_view = process_label(ss3_view,ss3_view)\n","ss8_label_view = process_label(ss8_view,ss8_view)\n","ss3_res_post = process_label(ss3_res_view,ss3_view)\n","ss8_res_post = process_label(ss8_res_view,ss8_view)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dd = {\n","    \"ss3_label\":ss3_label_view,\n","    \"ss3_result\":ss3_res_post,\n","    \"ss8_label\":ss8_label_view,\n","    \"ss8_result\":ss8_res_post\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(dd,\"./cb513_ss3_ss8_label_result.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# psi_ultra = process_label(psi_loss_view,ss3_view)\n","# phi_ultra = process_label(phi_loss_view,ss3_view)\n","# ss3_ultra = process_label(ss3_view,ss3_view)\n","# ss8_ultra = process_label(ss8_view,ss3_view)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# d = {\"psi_loss\":psi_ultra,\n","#      \"phi_loss\":phi_ultra,\n","#      \"ss3_origi_label\":ss3_ultra,\n","#      \"ss8_origi_label\":ss8_ultra,\n","# }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from pandas.core.frame import DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# data=DataFrame(d)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# df = data.to_csv('./ts115-violinplot.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# trainer = CustomTrainer(\n","#     model,\n","#     args,\n","#     eval_dataset=cb513_dataset,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","#     data_collator=data_collator,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# trainer = CustomTrainer(\n","#     model,\n","#     args,\n","#     eval_dataset=casp12_dataset,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","#     data_collator=data_collator,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# trainer = CustomTrainer(\n","#     model,\n","#     args,\n","#     eval_dataset=asp14_32_dataset,\n","#     tokenizer=tokenizer,\n","#     compute_metrics=compute_metrics,\n","#     data_collator=data_collator,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# trainer.evaluate()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
